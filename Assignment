# -*- coding: utf-8 -*-
"""
Niemand leest dit toch?
@author: nickt
"""

import numpy as np
import scipy
import scipy.linalg as la
import matplotlib.pyplot as plt
from astropy.table import Table

### Question 1.1 ###
print('\n ---------- Question 1.1 ---------- \n ')
# A1 PLU Factorization#

A1 = np.array([[7,2,1],[1,7,1],[3,2,7]])
P1,L1,U1 = la.lu(A1)

#plot the non-zero structure of both L and U in seperate figures using the functions plt.spy() and scipy.sparse.csr_matrix() #


pltL1 = plt.subplot(121)
pltL1.spy(scipy.sparse.csr_matrix(L1))
pltL1.set_title("L1",pad=15)
pltL1.set_xlabel('Column')
pltL1.set_ylabel('Row')

pltU1 = plt.subplot(122)
pltU1.spy(scipy.sparse.csr_matrix(U1))
pltU1.set_title("U1",pad=15)
pltU1.set_xlabel('Column')
pltU1.set_ylabel('Row')
plt.suptitle('Non-zero structure of both L and U')
plt.show()
#show that the product LU is indeed equal to A1#

LU1 = np.dot(L1,U1)
print('\n Matrix A1: \n', A1)
print('Product of L1U1: \n',LU1)

#A2 PLU Factorization#

A2 = np.array([[2,2,1],[1,1,1],[3,2,7]])
P2, L2, U2 = la.lu(A2)

#Compute the product LU#

LU2 = np.dot(L2,U2)

#Product PLU is indeed equal to A2#

PLU2 = np.dot(P2,LU2)
print('\n Matrix A2: \n', A2)
print('Product of PLU2: \n',PLU2)


### Question 1.2 ###
print('\n ---------- Question 1.2 ---------- \n ')
print('If s = c^T A^-1 d, substitute A^-1 d by x.  This gives s = c^T x. . With x = A^-1 d.') 
print('To avoid having to use the inverse of A explicitly, we formulate the matrix vector problem: Ax = d' )
print('Suppose, A = P L U, where L is a lower-triangular matrix,  U is an upper-triangular matrix and P is a permutation matrix')
print('We can compute s without the use of the inverse of A')
### Question 1.3 ###
print('\n ---------- Question 1.3 ---------- \n ')
#Give the coefficient matrix#

A = np.array([[6,0,-1,0,0], [-3,3,0,0,0], [0,-1,9,0,0], [0,-1,-8,11,-2], [-3,-1,0,0,4]])
print('Coefficient matrix A: \n', A)

#Given right-hand side vector#

b = np.array([50,0,160,0,0])
print('\n Right-hand side vector b: \n', b)

#Compute solution vector using the scipy.linsolve() function#

c = la.solve(A,b)
print('\n Solution vector c: \n', np.around(c, 3))

#Verify that the residual vector is equal in the computed solution#
r = b - np.dot(A,c)
print('\n Residual vector r:\n',  np.around(r, 5), '\n','Thus, r = 0 by approximation')

### Question 1.4 ###
print('\n ---------- Question 1.4 ---------- \n ')
#Compute for the three values of e listed, the condition number of the matrix A using the numpy.linalg.cond() function with the argument p=None.#
e = np.array([1, 0.1, 0.01])
f1 = np.array([1, 2, 3])
f2 = np.array([1, 2, 3.05])
ctab = np.array([])
sntab = np.array([])

for i in e:
    A = np.array([[1,2,3], [4,5,6], [8,10,12+i]])
    c = np.linalg.cond(A, p=None)
    u1 = la.solve(A,f1)
    u2 = la.solve(A,f2)
    print('For e = %g solution vector u1:' %i, '\n', u1)
    print('For e = %g solution vector u2:' %i, '\n', u2)
    ctab = np.append(ctab ,c)
    sn = np.divide(la.norm(u1-u2), la.norm(f1))
    sntab = np.append(sntab, sn)
    
print('\n')
print('Given in a table the value of e, the value of the corresponding condition number and the value of the scaled difference in norm of u1 and u2. \n')
t = Table([e, ctab, sntab], names=('e', 'Condition number', 'Scaled difference in norm of u1 & u2'))
print(t)

### Question 1.5 ###
print('\n ---------- Question 1.5 ---------- \n ')
A = np.array([[ 6,-1,-1], [ 6, 9, 1], [-3, 1,12]])
f = np.array([3,40,50])

#Solve the given linear system using the forward and backward Gauss-Seidel method method up.#
u0 = np.zeros(3)
k = 1
TOL = 10**(-6)
kmax = 100

def GaussSeidelForward(A, f, u0, TOL, k):
    rel_norm = la.norm(f-np.dot(A,u0))
    scal_norm = np.divide(rel_norm, rel_norm)
    scal_norm_list = [scal_norm]
    k_list = [0]
    u = np.copy(u0)
    while scal_norm > TOL and k < kmax:
        u[0] = (f[0] - A[0,1]*u[1] - A[0,2]*u[2])/A[0,0]
        u[1] = (f[1] - A[1,0]*u[0] - A[1,2]*u[2])/A[1,1]
        u[2] = (f[2] - A[2,0]*u[0] - A[2,1]*u[1])/A[2,2]
        resvec = f - np.dot(A,u)
        resnorm = np.linalg.norm(resvec)
        scal_norm = np.divide(resnorm, rel_norm)
        scal_norm_list.append(scal_norm)
        k_list.append(k)
        k += 1 
    return u, k_list, scal_norm_list

def GaussSeidelBackward(A, f, u0, TOL, k):
    rel_norm = la.norm(f-np.dot(A,u0))
    scal_norm = np.divide(rel_norm, rel_norm)
    scal_norm_list = [scal_norm]
    k_list = [0]
    u = np.copy(u0)
    while scal_norm > TOL and k < kmax:
        u[2] = (f[2] - A[2,0]*u[0] - A[2,1]*u[1])/A[2,2]
        u[1] = (f[1] - A[1,0]*u[0] - A[1,2]*u[2])/A[1,1]
        u[0] = (f[0] - A[0,1]*u[1] - A[0,2]*u[2])/A[0,0]
        resvec = f - np.dot(A,u)
        resnorm = np.linalg.norm(resvec)
        scal_norm = np.divide(resnorm, rel_norm)
        scal_norm_list.append(scal_norm)
        k_list.append(k)
        k += 1 
    return u, k_list, scal_norm_list

uf, k_listf, scal_norm_listf = GaussSeidelForward(A, f, u0, TOL, k)
ub, k_listb, scal_norm_listb = GaussSeidelBackward(A, f, u0, TOL, k)

plt.plot(k_listf, scal_norm_listf, label='forward')
plt.plot(k_listb, scal_norm_listb, label='backward')
plt.legend()
plt.yscale("log")
plt.xlabel("Iteration k")
plt.ylabel('Scaled residual norm')
plt.suptitle('Convergence history for the forward and backward Gauss-Seidel method')

    
        
